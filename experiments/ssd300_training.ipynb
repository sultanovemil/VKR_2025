{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u2aZ6WOCd-J"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Модуль для проведения K-Fold Cross-Validation модели SSD300 с VGG16.\n",
        "\n",
        "Содержит функции для:\n",
        "1. Загрузки и подготовки данных в формате COCO\n",
        "2. Разделения данных на K фолдов\n",
        "3. Обучения модели на каждом фолде\n",
        "4. Оценки качества модели с расчетом доверительных интервалов\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import SSD300_VGG16_Weights\n",
        "from torchvision.models.detection.ssd import SSDHead\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Класс для хранения конфигурации обучения.\"\"\"\n",
        "\n",
        "    # Пути к данным\n",
        "    TRAIN_ANNOTATIONS = \"/kaggle/working/kpk_dataset-9/train/_annotations.coco.json\"\n",
        "    TRAIN_DATA_DIR = \"/kaggle/working/kpk_dataset-9/train\"\n",
        "    VAL_ANNOTATIONS = \"/kaggle/working/kpk_dataset-9/valid/_annotations.coco.json\"\n",
        "    VAL_DATA_DIR = \"/kaggle/working/kpk_dataset-9/valid\"\n",
        "\n",
        "    # Параметры обучения\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.0005\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Параметры K-Fold\n",
        "    K_FOLDS = 5\n",
        "    CURRENT_FOLD = 1  # Текущий фолд для обучения\n",
        "\n",
        "\n",
        "class FoldCocoDataset(Dataset):\n",
        "    \"\"\"Кастомный датасет для работы с COCO аннотациями и K-Fold разбиением.\"\"\"\n",
        "\n",
        "    def __init__(self, root, annotation, image_ids, transforms=None):\n",
        "        \"\"\"\n",
        "        Инициализация датасета.\n",
        "\n",
        "        Args:\n",
        "            root (str): Путь к директории с изображениями\n",
        "            annotation (str): Путь к файлу аннотаций COCO\n",
        "            image_ids (list): Список ID изображений для включения в датасет\n",
        "            transforms (callable, optional): Трансформы для изображений\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation)\n",
        "        self.image_ids = image_ids\n",
        "        self.cat_ids = self.coco.getCatIds()\n",
        "        self.cat2label = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
        "\n",
        "        # Фильтруем изображения только из выбранного фолда\n",
        "        self.ids = [img_id for img_id in self.coco.imgs.keys()\n",
        "                   if img_id in image_ids]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Получение одного элемента датасета.\"\"\"\n",
        "        img_id = self.ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(self.cat2label[ann['category_id']])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([img_id])\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество элементов в датасете.\"\"\"\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "def get_transform():\n",
        "    \"\"\"Возвращает трансформы для изображений.\"\"\"\n",
        "    def transform(image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        image = F.normalize(\n",
        "            image,\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        return image, target\n",
        "    return transform\n",
        "\n",
        "\n",
        "def create_model(num_classes):\n",
        "    \"\"\"Создает модель SSD300 с VGG16 backbone.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Количество классов (включая background)\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: Модель SSD300\n",
        "    \"\"\"\n",
        "    model = torchvision.models.detection.ssd300_vgg16(\n",
        "        weights=SSD300_VGG16_Weights.DEFAULT\n",
        "    )\n",
        "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
        "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
        "    model.head = SSDHead(in_channels, num_anchors, num_classes)\n",
        "    return model.to(Config.DEVICE)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    \"\"\"Обучение модели на одной эпохе.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Модель для обучения\n",
        "        optimizer (torch.optim.Optimizer): Оптимизатор\n",
        "        data_loader (DataLoader): Загрузчик данных\n",
        "        device (torch.device): Устройство для обучения\n",
        "        epoch (int): Номер текущей эпохи\n",
        "\n",
        "    Returns:\n",
        "        float: Среднее значение функции потерь на эпохе\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for images, targets in progress:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "        progress.set_postfix({\"loss\": losses.item()})\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def create_kfold_split():\n",
        "    \"\"\"Создает K-Fold разбиение данных.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_image_ids, val_image_ids) - списки ID изображений\n",
        "    \"\"\"\n",
        "    # Загружаем аннотации\n",
        "    coco_train = COCO(Config.TRAIN_ANNOTATIONS)\n",
        "    coco_val = COCO(Config.VAL_ANNOTATIONS)\n",
        "\n",
        "    # Получаем все image ids\n",
        "    train_ids = list(sorted(coco_train.imgs.keys()))\n",
        "    val_ids = list(sorted(coco_val.imgs.keys()))\n",
        "    all_ids = train_ids + val_ids\n",
        "\n",
        "    # Создаем KFold разбиение\n",
        "    kf = KFold(n_splits=Config.K_FOLDS, shuffle=True, random_state=42)\n",
        "    splits = list(kf.split(all_ids))\n",
        "\n",
        "    # Получаем train/val индексы для текущего фолда\n",
        "    train_idx, val_idx = splits[Config.CURRENT_FOLD - 1]\n",
        "\n",
        "    # Получаем соответствующие image ids\n",
        "    train_image_ids = [all_ids[i] for i in train_idx]\n",
        "    val_image_ids = [all_ids[i] for i in val_idx]\n",
        "\n",
        "    return train_image_ids, val_image_ids\n",
        "\n",
        "\n",
        "def train_with_kfold():\n",
        "    \"\"\"Основной цикл обучения с K-Fold кросс-валидацией.\"\"\"\n",
        "    # Создаем K-Fold разбиение\n",
        "    train_image_ids, val_image_ids = create_kfold_split()\n",
        "\n",
        "    # Создаем датасеты для текущего фолда\n",
        "    train_dataset = FoldCocoDataset(\n",
        "        Config.TRAIN_DATA_DIR,\n",
        "        Config.TRAIN_ANNOTATIONS,\n",
        "        train_image_ids,\n",
        "        get_transform()\n",
        "    )\n",
        "\n",
        "    val_dataset = FoldCocoDataset(\n",
        "        Config.VAL_DATA_DIR,\n",
        "        Config.VAL_ANNOTATIONS,\n",
        "        val_image_ids,\n",
        "        get_transform()\n",
        "    )\n",
        "\n",
        "    num_classes = len(train_dataset.cat_ids) + 1  # +background\n",
        "    model = create_model(num_classes)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # Оптимизатор\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(params, lr=Config.LEARNING_RATE, weight_decay=0.0005)\n",
        "    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    # Обучение\n",
        "    for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
        "        train_loss = train_one_epoch(\n",
        "            model, optimizer, train_loader, Config.DEVICE, epoch\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Fold {Config.CURRENT_FOLD}, \"\n",
        "            f\"Epoch {epoch}/{Config.NUM_EPOCHS} - \"\n",
        "            f\"Train Loss: {train_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "    # Сохранение модели\n",
        "    os.makedirs(f\"fold_{Config.CURRENT_FOLD}\", exist_ok=True)\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        f\"fold_{Config.CURRENT_FOLD}/final_model.pth\"\n",
        "    )\n",
        "\n",
        "    print(f\"Обучение для фолда {Config.CURRENT_FOLD} завершено!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    \"\"\"Оценка модели на датасете с использованием метрик COCO.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Обученная модель\n",
        "        dataset (Dataset): Датасет для оценки\n",
        "\n",
        "    Returns:\n",
        "        np.array: Массив с метриками оценки\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "    coco = dataset.coco\n",
        "    cat_ids = coco.getCatIds()\n",
        "\n",
        "    for idx in tqdm(range(len(dataset))):\n",
        "        image, _ = dataset[idx]\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image.to(Config.DEVICE)])[0]\n",
        "\n",
        "        prediction = {k: v.cpu() for k, v in prediction.items()}\n",
        "        image_info = coco.loadImgs(dataset.ids[idx])[0]\n",
        "\n",
        "        for box, label, score in zip(\n",
        "            prediction['boxes'],\n",
        "            prediction['labels'],\n",
        "            prediction['scores']\n",
        "        ):\n",
        "            results.append({\n",
        "                \"image_id\": image_info['id'],\n",
        "                \"category_id\": cat_ids[label - 1],\n",
        "                \"bbox\": [\n",
        "                    box[0].item(),\n",
        "                    box[1].item(),\n",
        "                    (box[2] - box[0]).item(),\n",
        "                    (box[3] - box[1]).item()\n",
        "                ],\n",
        "                \"score\": score.item()\n",
        "            })\n",
        "\n",
        "    coco_pred = coco.loadRes(results)\n",
        "    coco_eval = COCOeval(coco, coco_pred, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval.stats\n",
        "\n",
        "\n",
        "def calculate_confidence_intervals(data, confidence=0.95):\n",
        "    \"\"\"Вычисляет среднее значение и доверительный интервал.\n",
        "\n",
        "    Args:\n",
        "        data (list or np.array): Массив значений\n",
        "        confidence (float): Уровень доверия (по умолчанию 0.95)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mean, (lower_bound, upper_bound))\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data, ddof=1)\n",
        "    n = len(data)\n",
        "\n",
        "    from scipy import stats\n",
        "    t = stats.t.ppf((1 + confidence) / 2., n - 1)\n",
        "    margin = t * std / np.sqrt(n)\n",
        "\n",
        "    return mean, (mean - margin, mean + margin)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Запуск кросс-валидации\n",
        "    model = train_with_kfold()\n",
        "\n",
        "    # Оценка модели\n",
        "    test_dataset = FoldCocoDataset(\n",
        "        Config.VAL_DATA_DIR,\n",
        "        Config.VAL_ANNOTATIONS,\n",
        "        list(range(100)),\n",
        "        get_transform()\n",
        "    )\n",
        "\n",
        "    metrics = evaluate_model(model, test_dataset)\n",
        "    print(\"Metrics:\", metrics)"
      ]
    }
  ]
}
